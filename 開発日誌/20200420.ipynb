{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Data loaded!!\n",
      "Training data amounts :3800\n",
      "Test data amounts :200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tensorflowによるオートエンコーダの実装\n",
    "思い出しがてらの作成なのでコメントが膨大…\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#%%\n",
    "# cording = UTF-8\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "import scipy\n",
    "import librosa #無くしたい\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "####################################初期化####################################\n",
    "aug_amount = 100    #ファイルごとのAugmentationの回数\n",
    "lr = 0.05          #初期学習率\n",
    "alpha = 0.0001       #L2正則化の係数\n",
    "dr_rate = 0.3       #ドロップアウト率\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "encode_dim = 1000   #オートエンコーダの圧縮次元\n",
    "\n",
    "#ディレクトリの初期化\n",
    "base_dir = \"../\"\n",
    "data_dir =os.path.join(base_dir,\"data\")\n",
    "ok_dir = os.path.join(data_dir,\"OK\")\n",
    "ng_dir = os.path.join(data_dir,\"NG\")\n",
    "env_dir = os.path.join(data_dir,\"environment\")\n",
    "\n",
    "#学習用データファイル\n",
    "datafile = \"dataset2.npz\"\n",
    "\n",
    "####################################関数定義###################################\n",
    "\n",
    "#対象ディレクトリのファイル一覧を取得\n",
    "def get_file_list(dir):\n",
    "    path = dir\n",
    "    file_list = os.listdir(path)\n",
    "    print(\"get file_list :{}\".format(file_list))\n",
    "    return file_list\n",
    "\n",
    "#対象ディレクトリの最大ファイルをサーチ\n",
    "def wav_search(dir,f_list):\n",
    "    #呼び出されるごとに初期化する\n",
    "    wave_list = []\n",
    "    file_size = 0\n",
    "    \n",
    "    return_path = os.path.abspath('./')\n",
    "    \n",
    "    os.chdir(dir)\n",
    "    for i in f_list:\n",
    "        search_index = re.search('.wav',i)\n",
    "        if search_index:\n",
    "            wave_list .append(i)\n",
    "            if os.path.getsize(i) > file_size:\n",
    "                file_size = os.path.getsize(i)\n",
    "                largest_file = i\n",
    "        \n",
    "    os.chdir(return_path)   #カレントディレクトリを戻す\n",
    "    print(\"get file :{0} ,file size:{1}\"\\\n",
    "        .format(largest_file,file_size))\n",
    "    return wave_list,largest_file,file_size\n",
    "\n",
    "#オーディオファイルの読み込み サンプルレート22.05kHz、モノラルで固定\n",
    "def load_wav(dir,file):\n",
    "    #呼び出されるごとに初期化する\n",
    "    wf = np.arange(0)\n",
    "\n",
    "    f_path = os.path.join(dir,file)\n",
    "    wf,sp_rate = librosa.load(f_path,sr=22050,mono = True)\n",
    "    del sp_rate\n",
    "    return wf\n",
    "\n",
    "#スペクトログラムの取得 パワースペクトラムのまま処理するならlibrosa不要\n",
    "def get_spg(wf):\n",
    "    spg = np.arange(0)\n",
    "    sp_f,sp_t,spg = scipy.signal.spectrogram(wf,fs=22050,\n",
    "        window = np.hamming(1024),nfft =1024)\n",
    "    spg = librosa.power_to_db(spg)\n",
    "    spg =spg.astype('float16')\n",
    "    return sp_f,sp_t,spg\n",
    "\n",
    "#Augmentationの処理\n",
    "def aug_process(frame,dir,wave_list,env_file,):\n",
    "    #呼び出されるごとに初期化する\n",
    "    length = 0\n",
    "    count = 0\n",
    "    wf = np.arange(0)\n",
    "\n",
    "    length = int(frame * 1.2)\n",
    "    for i in wave_list:\n",
    "        wf = load_wav(dir,i)\n",
    "        for j in range(aug_amount):\n",
    "            start = random.randint(0,len(env_file)-length)\n",
    "            aug_wav = copy.deepcopy(env_file[start : start + length])\n",
    "            del start\n",
    "            start = random.randint(0,len(aug_wav) - len(wf))\n",
    "            aug_wav = aug_wav + random.gauss(1,0.05)\n",
    "            aug_wav[ start:start + len(wf) ] = \\\n",
    "                aug_wav[ start : start + len(wf) ] + wf\n",
    "            sp_f,sp_t,spg = get_spg(aug_wav)\n",
    "            spg = spg.reshape(1,len(sp_f),len(sp_t))\n",
    "            try:\n",
    "                X_data\n",
    "            except:\n",
    "                X_data = copy.deepcopy(spg)\n",
    "            else:\n",
    "                X_data = np.vstack((X_data,spg))\n",
    "            del start,aug_wav,sp_f,sp_t,spg\n",
    "            count = count + 1\n",
    "        del wf\n",
    "        print(\"Augmentation done! total count = {}\".format(count))\n",
    "\n",
    "    return X_data\n",
    "\n",
    "#データセットの作成 ここまでの関数は全部ここに集約される\n",
    "#最大ファイルサイズに合わせてフレームサイズを定義し\n",
    "#OK・NG各データセットを作成後、結合する\n",
    "\n",
    "def new_dataset(aug,ok_dir,ng_dir,env_dir):\n",
    "    #OKNGそれぞれのファイルリストと最大ファイルを取得\n",
    "    ok_filelist = get_file_list(ok_dir)\n",
    "    ok_wave_list,ok_largeest_name,ok_largest_size = wav_search(ok_dir,ok_filelist)\n",
    "    ng_filelist = get_file_list(ng_dir)\n",
    "    ng_wave_list,ng_largeest_name,ng_largest_size = wav_search(ng_dir,ng_filelist)\n",
    "\n",
    "    #OKNGの最大を比較\n",
    "    if ok_largest_size>ng_largest_size:\n",
    "        largest_dir = ok_dir\n",
    "        lergest_name = ok_largeest_name\n",
    "        print(\"largetst:OK\")\n",
    "    else:\n",
    "        largest_dir = ng_dir\n",
    "        lergest_name = ng_largeest_name\n",
    "        print(\"largetst:NG\")\n",
    "\n",
    "    #最大フレームサイズを取得\n",
    "    wf = load_wav(largest_dir,lergest_name)\n",
    "    frame = int(len(wf))\n",
    "    #wf = np.insert(wf,frame,np.empty(int(frame*0.2))) #1.2倍する\n",
    "    #sp_f,sp_t,spg = get_spg(wf) \n",
    "    #X_initsize = (len(sp_f),len(sp_t))\n",
    "    #del wf,sp_f,sp_t,spg\n",
    "    del wf\n",
    "\n",
    "    #環境音データをロード\n",
    "    env_data = load_wav(env_dir,\"env.wav\")\n",
    "    \n",
    "    #OKデータセット作成\n",
    "    X_ok = copy.deepcopy(\n",
    "        aug_process(frame,ok_dir,ok_wave_list,env_data)\n",
    "        )\n",
    "    y_ok = np.zeros(len(X_ok),dtype = 'bool')\n",
    "\n",
    "    #NGデータセット作成\n",
    "    X_ng = copy.deepcopy(\n",
    "        aug_process(frame,ng_dir,ng_wave_list,env_data)\n",
    "        )\n",
    "    y_ng = np.ones(len(X_ng),dtype = 'bool')\n",
    "\n",
    "    #データセットの結合\n",
    "    X_data = np.vstack((X_ok,X_ng))\n",
    "    y_data = np.append(y_ok,y_ng)\n",
    "    del X_ok,y_ok,X_ng,y_ng\n",
    "\n",
    "    return X_data,y_data\n",
    "\n",
    "#OKNGが混在したデータからFalseのみを分離する\n",
    "def mixed_to_sprit(X_mixed,y_mixed):\n",
    "    #呼び出されるごとに初期化する\n",
    "    try:\n",
    "        X_sprit\n",
    "    except:\n",
    "        pass    #X_spritが存在しなければ何もしない\n",
    "    else:\n",
    "        del X_sprit #前のデータを消去する\n",
    "\n",
    "    for i in range(len(X_mixed)):\n",
    "        if y_mixed[i] == False:\n",
    "            try:\n",
    "                X_sprit\n",
    "            except: #X_spritを生成する\n",
    "                X_sprit = copy.deepcopy(X_mixed[i])\n",
    "                X_sprit = X_sprit.reshape(1,X_mixed.shape[1])\n",
    "            else:   #既存のX_spritに追加する 前段の例外処理はここを避けるため\n",
    "                X_sprit = np.vstack(\n",
    "                    (X_sprit,X_mixed[i].reshape(1,X_mixed.shape[1]))\n",
    "                    )\n",
    "\n",
    "    return X_sprit\n",
    "\n",
    "###################################メイン処理###################################\n",
    "\n",
    "#データセット読み込み なければ作る\n",
    "if os.path.exists(os.path.join(data_dir,datafile)) == False:\n",
    "    X_data,y_data = new_dataset(aug_amount,ok_dir,ng_dir,env_dir)\n",
    "    np.savez_compressed(os.path.join(data_dir,datafile),\n",
    "        X = X_data,y = y_data)\n",
    "    print(\"Data set saved!\") #ファイルネーム表示機能つけること\n",
    "else:\n",
    "    load_data = np.load(os.path.join(data_dir,datafile))\n",
    "    X_data =load_data['X']\n",
    "    y_data = load_data['y']\n",
    "    del load_data\n",
    "    print(\"Data loaded!!\")\n",
    "\n",
    "#データ前処理 trainとtestを分離\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_shape = X_data.shape[1:]\n",
    "X_data = X_data.reshape(len(X_data),-1) #アフィン変換\n",
    "X_train,X_test,y_train,y_test = \\\n",
    "    train_test_split(X_data,y_data,test_size=0.05)\n",
    "print(\n",
    "\"Training data amounts :{0}\\n\\\n",
    "Test data amounts :{1}\"\\\n",
    ".format(len(y_train),len(y_test))\n",
    ")\n",
    "del X_data,y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train for Autoencoder was splited!!\n",
      "amount/shape:(1906, 60021)\n"
     ]
    }
   ],
   "source": [
    "#X_trainからOKデータ(False)だけを抽出する\n",
    "\n",
    "X_train_ae = mixed_to_sprit(X_train,y_train)\n",
    "print(\n",
    "\"X_train for Autoencoder was splited!!\\n\\\n",
    "amount/shape:{0}\"\n",
    ".format(X_train_ae.shape)\n",
    ")\n",
    "\n",
    "#スケーラを定義する(AEの出力にシグモイドを使うため)\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#オートエンコーダの定義\n",
    "\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input, Dense,Dropout,Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "\n",
    "#Functionalでの実装\n",
    "def ae(input_dim,encode_dim,lr,alpha,dr_rate):\n",
    "\n",
    "    input_data = Input(shape = (input_dim,))\n",
    "\n",
    "    #エンコーダを定義\n",
    "    encoder = Dense(\n",
    "        encode_dim,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        kernel_regularizer=regularizers.l2(alpha),\n",
    "        )(input_data)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Dropout(dr_rate)(encoder)\n",
    "    encoder = Activation(\"relu\")(encoder)\n",
    "\n",
    "    #デコーダを定義 こっちにはドロップアウトは定義しない\n",
    "    decoder = Dense(input_dim,kernel_initializer=\"he_normal\")(encoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation(\"sigmoid\")(decoder)\n",
    "\n",
    "    #モデルを定義\n",
    "    autoencoder = Model(input = input_data,output = decoder)\n",
    "\n",
    "    #最適化関数\n",
    "    opt = keras.optimizers.nadam(lr = lr)\n",
    "\n",
    "    autoencoder.compile(\n",
    "        optimizer = opt,loss='binary_crossentropy',metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    return autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.7719 - accuracy: 0.0281\n",
      "Epoch 2/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.7151 - accuracy: 0.0315\n",
      "Epoch 3/100\n",
      "1906/1906 [==============================] - 104s 55ms/step - loss: 1.7372 - accuracy: 0.0315\n",
      "Epoch 4/100\n",
      "1906/1906 [==============================] - 102s 54ms/step - loss: 1.7670 - accuracy: 0.0315\n",
      "Epoch 5/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.6802 - accuracy: 0.0317\n",
      "Epoch 6/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.7148 - accuracy: 0.0316\n",
      "Epoch 7/100\n",
      "1906/1906 [==============================] - 103s 54ms/step - loss: 1.6663 - accuracy: 0.0316\n",
      "Epoch 8/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.6677 - accuracy: 0.0317\n",
      "Epoch 9/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.6359 - accuracy: 0.0318\n",
      "Epoch 10/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.6374 - accuracy: 0.0318\n",
      "Epoch 11/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.5996 - accuracy: 0.0317\n",
      "Epoch 12/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.6105 - accuracy: 0.0318\n",
      "Epoch 13/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.5666 - accuracy: 0.0318\n",
      "Epoch 14/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.5785 - accuracy: 0.0319\n",
      "Epoch 15/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.5447 - accuracy: 0.0317\n",
      "Epoch 16/100\n",
      "1906/1906 [==============================] - 104s 55ms/step - loss: 1.5386 - accuracy: 0.0317\n",
      "Epoch 17/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.5354 - accuracy: 0.0316\n",
      "Epoch 18/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.5014 - accuracy: 0.0316\n",
      "Epoch 19/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.5066 - accuracy: 0.0317\n",
      "Epoch 20/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.4914 - accuracy: 0.0315\n",
      "Epoch 21/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.4661 - accuracy: 0.0315\n",
      "Epoch 22/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.4722 - accuracy: 0.0316\n",
      "Epoch 23/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.4553 - accuracy: 0.0316\n",
      "Epoch 24/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.4366 - accuracy: 0.0316\n",
      "Epoch 25/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.4400 - accuracy: 0.0316\n",
      "Epoch 26/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.4243 - accuracy: 0.0317\n",
      "Epoch 27/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.4041 - accuracy: 0.0316\n",
      "Epoch 28/100\n",
      "1906/1906 [==============================] - 108s 57ms/step - loss: 1.4053 - accuracy: 0.0315\n",
      "Epoch 29/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.3984 - accuracy: 0.0316\n",
      "Epoch 30/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.3791 - accuracy: 0.0317\n",
      "Epoch 31/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.3708 - accuracy: 0.0316\n",
      "Epoch 32/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.3766 - accuracy: 0.0317\n",
      "Epoch 33/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.3602 - accuracy: 0.0316\n",
      "Epoch 34/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.3377 - accuracy: 0.0316\n",
      "Epoch 35/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.3451 - accuracy: 0.0316\n",
      "Epoch 36/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.3403 - accuracy: 0.0317\n",
      "Epoch 37/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.3229 - accuracy: 0.0315\n",
      "Epoch 38/100\n",
      "1906/1906 [==============================] - 104s 55ms/step - loss: 1.3113 - accuracy: 0.0315\n",
      "Epoch 39/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.3135 - accuracy: 0.0315\n",
      "Epoch 40/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.3046 - accuracy: 0.0315\n",
      "Epoch 41/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.2929 - accuracy: 0.0317\n",
      "Epoch 42/100\n",
      "1906/1906 [==============================] - 104s 55ms/step - loss: 1.2886 - accuracy: 0.0315\n",
      "Epoch 43/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.2851 - accuracy: 0.0316\n",
      "Epoch 44/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.2743 - accuracy: 0.0316\n",
      "Epoch 45/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.2674 - accuracy: 0.0315\n",
      "Epoch 46/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.2641 - accuracy: 0.0315\n",
      "Epoch 47/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.2567 - accuracy: 0.0315\n",
      "Epoch 48/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.2492 - accuracy: 0.0315\n",
      "Epoch 49/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.2424 - accuracy: 0.0316\n",
      "Epoch 50/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.2396 - accuracy: 0.0316\n",
      "Epoch 51/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.2322 - accuracy: 0.0316\n",
      "Epoch 52/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.2252 - accuracy: 0.0315\n",
      "Epoch 53/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.2214 - accuracy: 0.0316\n",
      "Epoch 54/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.2189 - accuracy: 0.0316\n",
      "Epoch 55/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.2086 - accuracy: 0.0317\n",
      "Epoch 56/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.2050 - accuracy: 0.0316\n",
      "Epoch 57/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.1998 - accuracy: 0.0316\n",
      "Epoch 58/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.1951 - accuracy: 0.0315\n",
      "Epoch 59/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.1877 - accuracy: 0.0315\n",
      "Epoch 60/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.1798 - accuracy: 0.0315\n",
      "Epoch 61/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.1791 - accuracy: 0.0315\n",
      "Epoch 62/100\n",
      "1906/1906 [==============================] - 109s 57ms/step - loss: 1.1772 - accuracy: 0.0316\n",
      "Epoch 63/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.1706 - accuracy: 0.0315\n",
      "Epoch 64/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.1634 - accuracy: 0.0316\n",
      "Epoch 65/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.1599 - accuracy: 0.0315\n",
      "Epoch 66/100\n",
      "1906/1906 [==============================] - 108s 57ms/step - loss: 1.1574 - accuracy: 0.0315\n",
      "Epoch 67/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.1541 - accuracy: 0.0316\n",
      "Epoch 68/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.1491 - accuracy: 0.0316\n",
      "Epoch 69/100\n",
      "1906/1906 [==============================] - 106s 55ms/step - loss: 1.1445 - accuracy: 0.0316\n",
      "Epoch 70/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.1422 - accuracy: 0.0317\n",
      "Epoch 71/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.1445 - accuracy: 0.0317\n",
      "Epoch 72/100\n",
      "1906/1906 [==============================] - 106s 55ms/step - loss: 1.1482 - accuracy: 0.0318\n",
      "Epoch 73/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.1369 - accuracy: 0.0320\n",
      "Epoch 74/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.1245 - accuracy: 0.0321\n",
      "Epoch 75/100\n",
      "1906/1906 [==============================] - 106s 56ms/step - loss: 1.1231 - accuracy: 0.0321\n",
      "Epoch 76/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.1188 - accuracy: 0.0321\n",
      "Epoch 77/100\n",
      "1906/1906 [==============================] - 103s 54ms/step - loss: 1.1170 - accuracy: 0.0322\n",
      "Epoch 78/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.1109 - accuracy: 0.0321\n",
      "Epoch 79/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.1100 - accuracy: 0.0322\n",
      "Epoch 80/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.1064 - accuracy: 0.0323\n",
      "Epoch 81/100\n",
      "1906/1906 [==============================] - 103s 54ms/step - loss: 1.1031 - accuracy: 0.0322\n",
      "Epoch 82/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.0981 - accuracy: 0.0322\n",
      "Epoch 83/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.1040 - accuracy: 0.0323\n",
      "Epoch 84/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.0997 - accuracy: 0.0323\n",
      "Epoch 85/100\n",
      "1906/1906 [==============================] - 103s 54ms/step - loss: 1.0937 - accuracy: 0.0323\n",
      "Epoch 86/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.0901 - accuracy: 0.0324\n",
      "Epoch 87/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.0861 - accuracy: 0.0323\n",
      "Epoch 88/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.0853 - accuracy: 0.0324\n",
      "Epoch 89/100\n",
      "1906/1906 [==============================] - 104s 54ms/step - loss: 1.0835 - accuracy: 0.0325\n",
      "Epoch 90/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.0837 - accuracy: 0.0325\n",
      "Epoch 91/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.0740 - accuracy: 0.0326\n",
      "Epoch 92/100\n",
      "1906/1906 [==============================] - 104s 55ms/step - loss: 1.0763 - accuracy: 0.0327\n",
      "Epoch 93/100\n",
      "1906/1906 [==============================] - 104s 55ms/step - loss: 1.0666 - accuracy: 0.0327\n",
      "Epoch 94/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.0604 - accuracy: 0.0327\n",
      "Epoch 95/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.0632 - accuracy: 0.0327\n",
      "Epoch 96/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.0622 - accuracy: 0.0327\n",
      "Epoch 97/100\n",
      "1906/1906 [==============================] - 104s 55ms/step - loss: 1.0512 - accuracy: 0.0327\n",
      "Epoch 98/100\n",
      "1906/1906 [==============================] - 105s 55ms/step - loss: 1.0486 - accuracy: 0.0327\n",
      "Epoch 99/100\n",
      "1906/1906 [==============================] - 107s 56ms/step - loss: 1.0417 - accuracy: 0.0328\n",
      "Epoch 100/100\n",
      "1906/1906 [==============================] - 108s 57ms/step - loss: 1.0450 - accuracy: 0.0327\n",
      "Autoencoder learning is over!\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60021)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              60022000  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60021)             60081021  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 60021)             240084    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60021)             0         \n",
      "=================================================================\n",
      "Total params: 120,347,105\n",
      "Trainable params: 120,225,063\n",
      "Non-trainable params: 122,042\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#オートエンコーダの学習\n",
    "ae = ae(\n",
    "    input_dim = X_train_ae.shape[1],\n",
    "    encode_dim = encode_dim,\n",
    "    lr = lr,\n",
    "    alpha = alpha,\n",
    "    dr_rate = dr_rate\n",
    ")                                       #インスタンス化\n",
    "\n",
    "X_train_ae = scaler.fit_transform(X_train_ae)   #スケール変換\n",
    "\n",
    "history = ae.fit(\n",
    "    X_train_ae,X_train_ae,\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "print('Autoencoder learning is over!')\n",
    "ae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習したオートエンコーダにx_testを通す\n",
    "X_test = scaler.transform(X_test)\n",
    "X_decode = ae.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまで正常に動作することを確認。ただし思った以上にaccuracyが上がらない…\n",
    "\n",
    "# 今後やること\n",
    "モデルの改良はここまでにして(時間がかかりすぎる) リファクタリングと機能追加を当面行う"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
