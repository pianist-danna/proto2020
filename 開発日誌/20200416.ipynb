{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Data loaded!!\n",
      "Training data amounts :3800\n",
      "Test data amounts :200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tensorflowによるオートエンコーダの実装\n",
    "思い出しがてらの作成なのでコメントが膨大…\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#%%\n",
    "# cording = UTF-8\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "import scipy\n",
    "import librosa #無くしたい\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "####################################初期化####################################\n",
    "aug_amount = 100    #ファイルごとのAugmentationの回数\n",
    "lr = 1e-01          #初期学習率\n",
    "alpha = 1e-03       #L2正則化の係数\n",
    "dr_rate = 0.3       #ドロップアウト率\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "encode_dim = 1000   #オートエンコーダの圧縮次元\n",
    "\n",
    "#ディレクトリの初期化\n",
    "base_dir = \"../\"\n",
    "data_dir =os.path.join(base_dir,\"data\")\n",
    "ok_dir = os.path.join(data_dir,\"OK\")\n",
    "ng_dir = os.path.join(data_dir,\"NG\")\n",
    "env_dir = os.path.join(data_dir,\"environment\")\n",
    "\n",
    "#学習用データファイル\n",
    "datafile = \"dataset2.npz\"\n",
    "\n",
    "####################################関数定義###################################\n",
    "\n",
    "#対象ディレクトリのファイル一覧を取得\n",
    "def get_file_list(dir):\n",
    "    path = dir\n",
    "    file_list = os.listdir(path)\n",
    "    print(\"get file_list :{}\".format(file_list))\n",
    "    return file_list\n",
    "\n",
    "#対象ディレクトリの最大ファイルをサーチ\n",
    "def wav_search(dir,f_list):\n",
    "    #呼び出されるごとに初期化する\n",
    "    wave_list = []\n",
    "    file_size = 0\n",
    "    \n",
    "    return_path = os.path.abspath('./')\n",
    "    \n",
    "    os.chdir(dir)\n",
    "    for i in f_list:\n",
    "        search_index = re.search('.wav',i)\n",
    "        if search_index:\n",
    "            wave_list .append(i)\n",
    "            if os.path.getsize(i) > file_size:\n",
    "                file_size = os.path.getsize(i)\n",
    "                largest_file = i\n",
    "        \n",
    "    os.chdir(return_path)   #カレントディレクトリを戻す\n",
    "    print(\"get file :{0} ,file size:{1}\"\\\n",
    "        .format(largest_file,file_size))\n",
    "    return wave_list,largest_file,file_size\n",
    "\n",
    "#オーディオファイルの読み込み サンプルレート22.05kHz、モノラルで固定\n",
    "def load_wav(dir,file):\n",
    "    #呼び出されるごとに初期化する\n",
    "    wf = np.arange(0)\n",
    "\n",
    "    f_path = os.path.join(dir,file)\n",
    "    wf,sp_rate = librosa.load(f_path,sr=22050,mono = True)\n",
    "    del sp_rate\n",
    "    return wf\n",
    "\n",
    "#スペクトログラムの取得 パワースペクトラムのまま処理するならlibrosa不要\n",
    "def get_spg(wf):\n",
    "    spg = np.arange(0)\n",
    "    sp_f,sp_t,spg = scipy.signal.spectrogram(wf,fs=22050,\n",
    "        window = np.hamming(1024),nfft =1024)\n",
    "    spg = librosa.power_to_db(spg)\n",
    "    spg =spg.astype('float16')\n",
    "    return sp_f,sp_t,spg\n",
    "\n",
    "#Augmentationの処理\n",
    "def aug_process(frame,dir,wave_list,env_file,):\n",
    "    #呼び出されるごとに初期化する\n",
    "    length = 0\n",
    "    count = 0\n",
    "    wf = np.arange(0)\n",
    "\n",
    "    length = int(frame * 1.2)\n",
    "    for i in wave_list:\n",
    "        wf = load_wav(dir,i)\n",
    "        for j in range(aug_amount):\n",
    "            start = random.randint(0,len(env_file)-length)\n",
    "            aug_wav = copy.deepcopy(env_file[start : start + length])\n",
    "            del start\n",
    "            start = random.randint(0,len(aug_wav) - len(wf))\n",
    "            aug_wav = aug_wav + random.gauss(1,0.05)\n",
    "            aug_wav[ start:start + len(wf) ] = \\\n",
    "                aug_wav[ start : start + len(wf) ] + wf\n",
    "            sp_f,sp_t,spg = get_spg(aug_wav)\n",
    "            spg = spg.reshape(1,len(sp_f),len(sp_t))\n",
    "            try:\n",
    "                X_data\n",
    "            except:\n",
    "                X_data = copy.deepcopy(spg)\n",
    "            else:\n",
    "                X_data = np.vstack((X_data,spg))\n",
    "            del start,aug_wav,sp_f,sp_t,spg\n",
    "            count = count + 1\n",
    "        del wf\n",
    "        print(\"Augmentation done! total count = {}\".format(count))\n",
    "\n",
    "    return X_data\n",
    "\n",
    "#データセットの作成 ここまでの関数は全部ここに集約される\n",
    "#最大ファイルサイズに合わせてフレームサイズを定義し\n",
    "#OK・NG各データセットを作成後、結合する\n",
    "\n",
    "def new_dataset(aug,ok_dir,ng_dir,env_dir):\n",
    "    #OKNGそれぞれのファイルリストと最大ファイルを取得\n",
    "    ok_filelist = get_file_list(ok_dir)\n",
    "    ok_wave_list,ok_largeest_name,ok_largest_size = wav_search(ok_dir,ok_filelist)\n",
    "    ng_filelist = get_file_list(ng_dir)\n",
    "    ng_wave_list,ng_largeest_name,ng_largest_size = wav_search(ng_dir,ng_filelist)\n",
    "\n",
    "    #OKNGの最大を比較\n",
    "    if ok_largest_size>ng_largest_size:\n",
    "        largest_dir = ok_dir\n",
    "        lergest_name = ok_largeest_name\n",
    "        print(\"largetst:OK\")\n",
    "    else:\n",
    "        largest_dir = ng_dir\n",
    "        lergest_name = ng_largeest_name\n",
    "        print(\"largetst:NG\")\n",
    "\n",
    "    #最大フレームサイズを取得\n",
    "    wf = load_wav(largest_dir,lergest_name)\n",
    "    frame = int(len(wf))\n",
    "    #wf = np.insert(wf,frame,np.empty(int(frame*0.2))) #1.2倍する\n",
    "    #sp_f,sp_t,spg = get_spg(wf) \n",
    "    #X_initsize = (len(sp_f),len(sp_t))\n",
    "    #del wf,sp_f,sp_t,spg\n",
    "    del wf\n",
    "\n",
    "    #環境音データをロード\n",
    "    env_data = load_wav(env_dir,\"env.wav\")\n",
    "    \n",
    "    #OKデータセット作成\n",
    "    X_ok = copy.deepcopy(\n",
    "        aug_process(frame,ok_dir,ok_wave_list,env_data)\n",
    "        )\n",
    "    y_ok = np.zeros(len(X_ok),dtype = 'bool')\n",
    "\n",
    "    #NGデータセット作成\n",
    "    X_ng = copy.deepcopy(\n",
    "        aug_process(frame,ng_dir,ng_wave_list,env_data)\n",
    "        )\n",
    "    y_ng = np.ones(len(X_ng),dtype = 'bool')\n",
    "\n",
    "    #データセットの結合\n",
    "    X_data = np.vstack((X_ok,X_ng))\n",
    "    y_data = np.append(y_ok,y_ng)\n",
    "    del X_ok,y_ok,X_ng,y_ng\n",
    "\n",
    "    return X_data,y_data\n",
    "\n",
    "#OKNGが混在したデータからFalseのみを分離する\n",
    "def mixed_to_sprit(X_mixed,y_mixed):\n",
    "    #呼び出されるごとに初期化する\n",
    "    try:\n",
    "        X_sprit\n",
    "    except:\n",
    "        pass    #X_spritが存在しなければ何もしない\n",
    "    else:\n",
    "        del X_sprit #前のデータを消去する\n",
    "\n",
    "    for i in range(len(X_mixed)):\n",
    "        if y_mixed[i] == False:\n",
    "            try:\n",
    "                X_sprit\n",
    "            except: #X_spritを生成する\n",
    "                X_sprit = copy.deepcopy(X_mixed[i])\n",
    "                X_sprit = X_sprit.reshape(1,X_mixed.shape[1])\n",
    "            else:   #既存のX_spritに追加する 前段の例外処理はここを避けるため\n",
    "                X_sprit = np.vstack(\n",
    "                    (X_sprit,X_mixed[i].reshape(1,X_mixed.shape[1]))\n",
    "                    )\n",
    "\n",
    "    return X_sprit\n",
    "\n",
    "###################################メイン処理###################################\n",
    "\n",
    "#データセット読み込み なければ作る\n",
    "if os.path.exists(os.path.join(data_dir,datafile)) == False:\n",
    "    X_data,y_data = new_dataset(aug_amount,ok_dir,ng_dir,env_dir)\n",
    "    np.savez_compressed(os.path.join(data_dir,datafile),\n",
    "        X = X_data,y = y_data)\n",
    "    print(\"Data set saved!\") #ファイルネーム表示機能つけること\n",
    "else:\n",
    "    load_data = np.load(os.path.join(data_dir,datafile))\n",
    "    X_data =load_data['X']\n",
    "    y_data = load_data['y']\n",
    "    del load_data\n",
    "    print(\"Data loaded!!\")\n",
    "\n",
    "#データ前処理 trainとtestを分離\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_shape = X_data.shape[1:]\n",
    "X_data = X_data.reshape(len(X_data),-1) #アフィン変換\n",
    "X_train,X_test,y_train,y_test = \\\n",
    "    train_test_split(X_data,y_data,test_size=0.05)\n",
    "print(\n",
    "\"Training data amounts :{0}\\n\\\n",
    "Test data amounts :{1}\"\\\n",
    ".format(len(y_train),len(y_test))\n",
    ")\n",
    "del X_data,y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train for Autoencoder was splited!!\n",
      "amount/shape:(1901, 60021)\n"
     ]
    }
   ],
   "source": [
    "#X_trainからOKデータ(False)だけを抽出する\n",
    "\n",
    "X_train_ae = mixed_to_sprit(X_train,y_train)\n",
    "print(\n",
    "\"X_train for Autoencoder was splited!!\\n\\\n",
    "amount/shape:{0}\"\n",
    ".format(X_train_ae.shape)\n",
    ")\n",
    "\n",
    "#スケーラを定義する(AEの出力にシグモイドを使うため)\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1901/1901 [==============================] - 114s 60ms/step - loss: 0.8847 - accuracy: 0.0200\n",
      "Epoch 2/100\n",
      "1901/1901 [==============================] - 114s 60ms/step - loss: 0.7713 - accuracy: 0.0216\n",
      "Epoch 3/100\n",
      "1901/1901 [==============================] - 110s 58ms/step - loss: 0.7663 - accuracy: 0.0229\n",
      "Epoch 4/100\n",
      "1901/1901 [==============================] - 110s 58ms/step - loss: 0.7515 - accuracy: 0.0244\n",
      "Epoch 5/100\n",
      "1901/1901 [==============================] - 107s 56ms/step - loss: 0.7410 - accuracy: 0.0259\n",
      "Epoch 6/100\n",
      "1901/1901 [==============================] - 110s 58ms/step - loss: 0.7280 - accuracy: 0.0277\n",
      "Epoch 7/100\n",
      "1901/1901 [==============================] - 109s 58ms/step - loss: 0.7355 - accuracy: 0.0296\n",
      "Epoch 8/100\n",
      "1901/1901 [==============================] - 112s 59ms/step - loss: 0.7136 - accuracy: 0.0311\n",
      "Epoch 9/100\n",
      "1901/1901 [==============================] - 111s 59ms/step - loss: 0.7097 - accuracy: 0.0322\n",
      "Epoch 10/100\n",
      "1901/1901 [==============================] - 110s 58ms/step - loss: 0.7035 - accuracy: 0.0328\n",
      "Epoch 11/100\n",
      "1901/1901 [==============================] - 108s 57ms/step - loss: 0.7000 - accuracy: 0.0331\n",
      "Epoch 12/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.6977 - accuracy: 0.0333\n",
      "Epoch 13/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.7019 - accuracy: 0.0334\n",
      "Epoch 14/100\n",
      "1901/1901 [==============================] - 111s 59ms/step - loss: 0.6947 - accuracy: 0.0334\n",
      "Epoch 15/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.6896 - accuracy: 0.0335\n",
      "Epoch 16/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.6884 - accuracy: 0.0335\n",
      "Epoch 17/100\n",
      "1901/1901 [==============================] - 109s 57ms/step - loss: 0.6868 - accuracy: 0.0335\n",
      "Epoch 18/100\n",
      "1901/1901 [==============================] - 110s 58ms/step - loss: 0.6884 - accuracy: 0.0335\n",
      "Epoch 19/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.6900 - accuracy: 0.0335\n",
      "Epoch 20/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.6875 - accuracy: 0.0335\n",
      "Epoch 21/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.6877 - accuracy: 0.0335\n",
      "Epoch 22/100\n",
      "1901/1901 [==============================] - 111s 59ms/step - loss: 0.6879 - accuracy: 0.0335\n",
      "Epoch 23/100\n",
      "1901/1901 [==============================] - 109s 57ms/step - loss: 0.6975 - accuracy: 0.0335\n",
      "Epoch 24/100\n",
      "1901/1901 [==============================] - 109s 57ms/step - loss: 0.6861 - accuracy: 0.0335\n",
      "Epoch 25/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.6926 - accuracy: 0.0335\n",
      "Epoch 26/100\n",
      "1901/1901 [==============================] - 111s 59ms/step - loss: 0.6891 - accuracy: 0.0335\n",
      "Epoch 27/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.6893 - accuracy: 0.0335\n",
      "Epoch 28/100\n",
      "1901/1901 [==============================] - 112s 59ms/step - loss: 0.6921 - accuracy: 0.0335\n",
      "Epoch 29/100\n",
      "1901/1901 [==============================] - 110s 58ms/step - loss: 0.6829 - accuracy: 0.0335\n",
      "Epoch 30/100\n",
      "1901/1901 [==============================] - 108s 57ms/step - loss: 0.6847 - accuracy: 0.0334\n",
      "Epoch 31/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.6809 - accuracy: 0.0335\n",
      "Epoch 32/100\n",
      "1901/1901 [==============================] - 111s 58ms/step - loss: 0.6854 - accuracy: 0.0335\n",
      "Epoch 33/100\n",
      "1901/1901 [==============================] - 113s 60ms/step - loss: 0.6800 - accuracy: 0.0335\n",
      "Epoch 34/100\n",
      "1901/1901 [==============================] - 113s 59ms/step - loss: 0.6850 - accuracy: 0.0335\n",
      "Epoch 35/100\n",
      "1901/1901 [==============================] - 111s 59ms/step - loss: 0.6829 - accuracy: 0.0335\n",
      "Epoch 36/100\n",
      "1901/1901 [==============================] - 112s 59ms/step - loss: 0.6919 - accuracy: 0.0335\n",
      "Epoch 37/100\n",
      "1901/1901 [==============================] - 109s 58ms/step - loss: 0.6903 - accuracy: 0.0335\n",
      "Epoch 38/100\n",
      "1901/1901 [==============================] - 113s 59ms/step - loss: 0.6800 - accuracy: 0.0335\n",
      "Epoch 39/100\n",
      "1901/1901 [==============================] - 111s 59ms/step - loss: 0.6842 - accuracy: 0.0335\n",
      "Epoch 40/100\n",
      "1901/1901 [==============================] - 112s 59ms/step - loss: 0.6826 - accuracy: 0.0335\n",
      "Epoch 41/100\n",
      "1901/1901 [==============================] - 111s 59ms/step - loss: 0.6814 - accuracy: 0.0335\n",
      "Epoch 42/100\n",
      "1901/1901 [==============================] - 111s 59ms/step - loss: 0.6838 - accuracy: 0.0335\n",
      "Epoch 43/100\n",
      "1901/1901 [==============================] - 113s 59ms/step - loss: 0.6816 - accuracy: 0.0335\n",
      "Epoch 44/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6827 - accuracy: 0.0335\n",
      "Epoch 45/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6825 - accuracy: 0.0335\n",
      "Epoch 46/100\n",
      "1901/1901 [==============================] - 121s 64ms/step - loss: 0.6902 - accuracy: 0.0335\n",
      "Epoch 47/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6814 - accuracy: 0.0335\n",
      "Epoch 48/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6820 - accuracy: 0.0335\n",
      "Epoch 49/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6820 - accuracy: 0.0335\n",
      "Epoch 50/100\n",
      "1901/1901 [==============================] - 121s 63ms/step - loss: 0.6873 - accuracy: 0.0335\n",
      "Epoch 51/100\n",
      "1901/1901 [==============================] - 122s 64ms/step - loss: 0.6801 - accuracy: 0.0335\n",
      "Epoch 52/100\n",
      "1901/1901 [==============================] - 122s 64ms/step - loss: 0.6839 - accuracy: 0.0335\n",
      "Epoch 53/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6823 - accuracy: 0.0335\n",
      "Epoch 54/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6809 - accuracy: 0.0335\n",
      "Epoch 55/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6790 - accuracy: 0.0335\n",
      "Epoch 56/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6777 - accuracy: 0.0335\n",
      "Epoch 57/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6796 - accuracy: 0.0335\n",
      "Epoch 58/100\n",
      "1901/1901 [==============================] - 119s 62ms/step - loss: 0.6849 - accuracy: 0.0335\n",
      "Epoch 59/100\n",
      "1901/1901 [==============================] - 118s 62ms/step - loss: 0.6779 - accuracy: 0.0335\n",
      "Epoch 60/100\n",
      "1901/1901 [==============================] - 118s 62ms/step - loss: 0.6807 - accuracy: 0.0335\n",
      "Epoch 61/100\n",
      "1901/1901 [==============================] - 121s 64ms/step - loss: 0.6799 - accuracy: 0.0335\n",
      "Epoch 62/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6774 - accuracy: 0.0335\n",
      "Epoch 63/100\n",
      "1901/1901 [==============================] - 121s 63ms/step - loss: 0.6784 - accuracy: 0.0335\n",
      "Epoch 64/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6788 - accuracy: 0.0335\n",
      "Epoch 65/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6801 - accuracy: 0.0335\n",
      "Epoch 66/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6780 - accuracy: 0.0335\n",
      "Epoch 67/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6792 - accuracy: 0.0335\n",
      "Epoch 68/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6793 - accuracy: 0.0335\n",
      "Epoch 69/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6897 - accuracy: 0.0335\n",
      "Epoch 70/100\n",
      "1901/1901 [==============================] - 119s 62ms/step - loss: 0.6753 - accuracy: 0.0336\n",
      "Epoch 71/100\n",
      "1901/1901 [==============================] - 118s 62ms/step - loss: 0.6762 - accuracy: 0.0335\n",
      "Epoch 72/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6759 - accuracy: 0.0336\n",
      "Epoch 73/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6755 - accuracy: 0.0335\n",
      "Epoch 74/100\n",
      "1901/1901 [==============================] - 119s 62ms/step - loss: 0.6779 - accuracy: 0.0335\n",
      "Epoch 75/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6761 - accuracy: 0.0335\n",
      "Epoch 76/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6751 - accuracy: 0.0335\n",
      "Epoch 77/100\n",
      "1901/1901 [==============================] - 118s 62ms/step - loss: 0.6743 - accuracy: 0.0335\n",
      "Epoch 78/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6772 - accuracy: 0.0336\n",
      "Epoch 79/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6760 - accuracy: 0.0336\n",
      "Epoch 80/100\n",
      "1901/1901 [==============================] - 119s 62ms/step - loss: 0.6737 - accuracy: 0.0336\n",
      "Epoch 81/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6743 - accuracy: 0.0336\n",
      "Epoch 82/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6742 - accuracy: 0.0336\n",
      "Epoch 83/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6746 - accuracy: 0.0336\n",
      "Epoch 84/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6743 - accuracy: 0.0336\n",
      "Epoch 85/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6735 - accuracy: 0.0336\n",
      "Epoch 86/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6745 - accuracy: 0.0336\n",
      "Epoch 87/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6738 - accuracy: 0.0336\n",
      "Epoch 88/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6727 - accuracy: 0.0336\n",
      "Epoch 89/100\n",
      "1901/1901 [==============================] - 119s 63ms/step - loss: 0.6732 - accuracy: 0.0336\n",
      "Epoch 90/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6741 - accuracy: 0.0336\n",
      "Epoch 91/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6736 - accuracy: 0.0336\n",
      "Epoch 92/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6747 - accuracy: 0.0336\n",
      "Epoch 93/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6734 - accuracy: 0.0336\n",
      "Epoch 94/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6731 - accuracy: 0.0336\n",
      "Epoch 95/100\n",
      "1901/1901 [==============================] - 122s 64ms/step - loss: 0.6735 - accuracy: 0.0336\n",
      "Epoch 96/100\n",
      "1901/1901 [==============================] - 120s 63ms/step - loss: 0.6736 - accuracy: 0.0336\n",
      "Epoch 97/100\n",
      "1901/1901 [==============================] - 123s 64ms/step - loss: 0.6726 - accuracy: 0.0336\n",
      "Epoch 98/100\n",
      "1901/1901 [==============================] - 125s 66ms/step - loss: 0.6732 - accuracy: 0.0336\n",
      "Epoch 99/100\n",
      "1901/1901 [==============================] - 125s 66ms/step - loss: 0.6727 - accuracy: 0.0336\n",
      "Epoch 100/100\n",
      "1901/1901 [==============================] - 125s 66ms/step - loss: 0.6727 - accuracy: 0.0336\n",
      "Autoencoder learning is over!\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60021)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              60022000  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60021)             60081021  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 60021)             240084    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60021)             0         \n",
      "=================================================================\n",
      "Total params: 120,347,105\n",
      "Trainable params: 120,225,063\n",
      "Non-trainable params: 122,042\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#オートエンコーダの定義\n",
    "\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input, Dense,Dropout,Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "\n",
    "#Modelでの実装 もう少しわかりやすい形にする\n",
    "def ae(input_dim,encode_dim,lr =lr,alpha =alpha,dr_rate = dr_rate):\n",
    "\n",
    "    input_data = Input(shape = (input_dim,))\n",
    "\n",
    "    #エンコーダを定義\n",
    "    encoder = Dense(\n",
    "        encode_dim,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        kernel_regularizer=regularizers.l2(alpha),\n",
    "        )(input_data)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Dropout(dr_rate)(encoder)\n",
    "    encoder = Activation(\"relu\")(encoder)\n",
    "\n",
    "    #デコーダを定義 こっちにはドロップアウトは定義しない\n",
    "    decoder = Dense(input_dim,kernel_initializer=\"he_normal\")(encoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation(\"sigmoid\")(decoder)\n",
    "\n",
    "    #モデルを定義\n",
    "    autoencoder = Model(input = input_data,output = decoder)\n",
    "\n",
    "    #最適化関数 Nadamをデフォルトパラメタで使う 必要ならlr = lr を追記\n",
    "    opt = keras.optimizers.nadam()\n",
    "\n",
    "    autoencoder.compile(\n",
    "        optimizer = opt,loss='binary_crossentropy',metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "#オートエンコーダの学習\n",
    "ae = ae(\n",
    "    input_dim = X_train_ae.shape[1],\n",
    "    encode_dim = encode_dim\n",
    ")                                       #インスタンス化\n",
    "\n",
    "X_train_ae = scaler.fit_transform(X_train_ae)     #スケール変換\n",
    "\n",
    "ae.fit(\n",
    "    X_train_ae,X_train_ae,\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "print('Autoencoder learning is over!')\n",
    "ae.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# わかったこと\n",
    "\n",
    "- Scalerの導入で学習するようになった\n",
    "- デフォルトパラメタのままでは20epoch程度で学習が止まってしまう\n",
    "- fitを変数定義しないと学習曲線を残せない(Kerasの仕様？)\n",
    "\n",
    "# 次にやること\n",
    "- sklearnのラッパーでくるんでグリッドサーチを行う epochは20でいい\n",
    "- aeに渡す引数をちゃんとそろえる"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
